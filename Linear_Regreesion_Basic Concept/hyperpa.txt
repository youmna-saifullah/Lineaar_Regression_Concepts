| Hyperparameter           | What it Controls                                 | Effect on Loss                                                                                                                                        | Summary                                                     |
| ------------------------ | ------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- |
| **1. Learning Rate (Î±)** | How **big each step** is during gradient descent | - Too **high** â†’ overshoots minimum â†’ unstable loss (may even increase!)<br>- Too **low** â†’ slow convergence â†’ takes many steps to reduce loss        | ğŸ¯ Needs to be **just right** to smoothly reach lowest loss |
| **2. Epochs**            | How many **times you go over** the whole dataset | - Too **few** â†’ model may stop early â†’ **high loss**<br>- More epochs â†’ better training â†’ **lower loss**<br>- Too many â†’ might **overfit** (memorize) | ğŸ” More epochs = **more chances** to reduce loss            |
| **3. Batch Size**        | How many samples are used per update             | - **Small batch** (e.g. 1) â†’ fast updates, but noisy loss<br>- **Large batch** (e.g. full dataset) â†’ smoother, but slower                             | ğŸ¯ Batch size affects **stability + speed** of learning     |
