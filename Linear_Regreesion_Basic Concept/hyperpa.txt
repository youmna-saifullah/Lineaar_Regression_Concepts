| Hyperparameter           | What it Controls                                 | Effect on Loss                                                                                                                                        | Summary                                                     |
| ------------------------ | ------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- |
| **1. Learning Rate (α)** | How **big each step** is during gradient descent | - Too **high** → overshoots minimum → unstable loss (may even increase!)<br>- Too **low** → slow convergence → takes many steps to reduce loss        | 🎯 Needs to be **just right** to smoothly reach lowest loss |
| **2. Epochs**            | How many **times you go over** the whole dataset | - Too **few** → model may stop early → **high loss**<br>- More epochs → better training → **lower loss**<br>- Too many → might **overfit** (memorize) | 🔁 More epochs = **more chances** to reduce loss            |
| **3. Batch Size**        | How many samples are used per update             | - **Small batch** (e.g. 1) → fast updates, but noisy loss<br>- **Large batch** (e.g. full dataset) → smoother, but slower                             | 🎯 Batch size affects **stability + speed** of learning     |
